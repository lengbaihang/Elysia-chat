# Elysia-chat
è®©çˆ±è‰å¸Œé›…æŠšæ…°ä½ çš„å¿ƒå§â™ª



[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/apps/detail/lengbaihang1/Elysia) ![Static Badge](https://img.shields.io/badge/license-Apache--2.0-green?label=license)

[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/usercenter/lengbaihang1?vtab=create&module=models)

![v2-53f9a387bba7b2f673ffb242934afdc3_r](https://github.com/lengbaihang/Elysia-chat/assets/96370602/27967b2e-8ccb-410a-9c21-ee150f5e65b3)
![d2f9d3e7d0eb80f423a441430db69446_5839222736512585157](https://github.com/lengbaihang/Elysia-chat/assets/96370602/c0221a2d-c07d-4ebe-89d3-a76f1d516a85)
![v2-57874c12f9588ffbff916353481e41d3_r](https://github.com/lengbaihang/Elysia-chat/assets/96370602/0ccbd8b2-af8e-4bc3-954b-cbbe9a50b58d)

è¿™æ˜¯æˆ‘çš„ä¸€å°æ­¥ï¼Œå´æ˜¯çˆ±é—¨çš„ä¸€å¤§æ­¥

              *Introduction**


| åŸºç¡€å±æ€§ | æ¨¡å‹ç›¸å…³è§£é‡Š |
|:-------:|:-------:|
| æ¨¡å‹åŸºåº§ | InternLM2-Chat-7b |
| å¾®è°ƒæ–¹æ³• | QLoRA |
| æŠ€æœ¯åº“ | Xtuner + Transformers |


å‚è€ƒå¹¶å­¦ä¹ å¤§ä½¬çš„ç»éªŒï¼š

                    https://github.com/InternLM/tutorial/blob/main/xtuner/self.md

                    https://github.com/InternLM/tutorial/blob/main/xtuner/README.md
                     
                    https://github.com/SaaRaaS-1300/InternLM_openNotebook/tree/main/Horowag_7b
                     
                    https://datawhalechina.github.io/llm-universe/#/
                    
                    https://github.com/InternLM/xtuner
                    
                    https://github.com/Plachtaa/VITS-fast-fine-tuning/blob/main/README_ZH.md
                    



å¾®è°ƒç¯å¢ƒå‡†å¤‡

## ğŸ› ï¸ å¿«é€Ÿä¸Šæ‰‹

### å®‰è£…

- æ¨èä½¿ç”¨ conda å…ˆæ„å»ºä¸€ä¸ª Python-3.10 çš„è™šæ‹Ÿç¯å¢ƒ

  ```bash
  conda create --name xtuner-env python=3.10 -y
  conda activate xtuner-env
  ```

- é€šè¿‡ pip å®‰è£… XTunerï¼š

  ```shell
  pip install -U xtuner
  ```

  äº¦å¯é›†æˆ DeepSpeed å®‰è£…ï¼š

  ```shell
  pip install -U 'xtuner[deepspeed]'
  ```

- ä»æºç å®‰è£… XTunerï¼š

  ```shell
  git clone https://github.com/InternLM/xtuner.git
  cd xtuner
  pip install -e '.[all]'
  ```

### å¾®è°ƒ

XTuner æ”¯æŒå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ã€‚æ•°æ®é›†é¢„å¤„ç†æŒ‡å—è¯·æŸ¥é˜…[æ–‡æ¡£](./docs/zh_cn/user_guides/dataset_prepare.md)ã€‚

- **æ­¥éª¤ 0**ï¼Œå‡†å¤‡é…ç½®æ–‡ä»¶ã€‚XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

  ```shell
  xtuner list-cfg
  ```

  æˆ–è€…ï¼Œå¦‚æœæ‰€æä¾›çš„é…ç½®æ–‡ä»¶ä¸èƒ½æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œè¯·å¯¼å‡ºæ‰€æä¾›çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œç›¸åº”æ›´æ”¹ï¼š

  ```shell
  xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
  vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
  ```

- **æ­¥éª¤ 1**ï¼Œå¼€å§‹å¾®è°ƒã€‚

  ```shell
  xtuner train ${CONFIG_NAME_OR_PATH}
  ```

  ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ QLoRA ç®—æ³•åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ InternLM2-Chat-7Bï¼š

  ```shell
  # å•å¡
  xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
  # å¤šå¡
  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2
  (SLURM) srun ${SRUN_ARGS} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2
  ```

  - `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

  - æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥é˜…[æ–‡æ¡£](./docs/zh_cn/user_guides/finetune.md)ã€‚

- **æ­¥éª¤ 2**ï¼Œå°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼š

  ```shell
  xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
  ```

### å¯¹è¯

XTuner æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹è¯çš„å·¥å…·ã€‚

```shell
xtuner chat ${NAME_OR_PATH_TO_LLM} --adapter {NAME_OR_PATH_TO_ADAPTER} [optional arguments]
```

ä¾‹å¦‚ï¼š

ä¸ InternLM2-Chat-7B, oasst1 adapter å¯¹è¯ï¼š

```shell
xtuner chat internlm/internlm2-chat-7b --adapter xtuner/internlm2-chat-7b-qlora-oasst1 --prompt-template internlm2_chat
```

ä¸ LLaVA-InternLM2-7B å¯¹è¯ï¼š

```shell
xtuner chat internlm/internlm2-chat-7b --visual-encoder openai/clip-vit-large-patch14-336 --llava xtuner/llava-internlm2-7b --prompt-template internlm2_chat --image $IMAGE_PATH
```

æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥é˜…[æ–‡æ¡£](./docs/zh_cn/user_guides/chat.md)ã€‚

### éƒ¨ç½²

- **æ­¥éª¤ 0**ï¼Œå°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š

  ```shell
  xtuner convert merge \
      ${NAME_OR_PATH_TO_LLM} \
      ${NAME_OR_PATH_TO_ADAPTER} \
      ${SAVE_PATH} \
      --max-shard-size 2GB
  ```

- **æ­¥éª¤ 1**ï¼Œä½¿ç”¨ä»»æ„æ¨ç†æ¡†æ¶éƒ¨ç½²å¾®è°ƒåçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚ [LMDeploy](https://github.com/InternLM/lmdeploy) ğŸš€ï¼š

  ```shell
  pip install lmdeploy
  python -m lmdeploy.pytorch.chat ${NAME_OR_PATH_TO_LLM} \
      --max_new_tokens 256 \
      --temperture 0.8 \
      --top_p 0.95 \
      --seed 0
  ```








æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢è¿™å¼ å›¾æ¥ç®€å•äº†è§£ä¸€ä¸‹ XTuner çš„è¿è¡ŒåŸç†ã€‚

<img width="3216" alt="XTunerFlow1" src="https://github.com/InternLM/Tutorial/assets/108343727/0c4817e8-ddaf-4276-ad16-b65d5ec6b4ae">





> å‡å¦‚é€Ÿåº¦å¤ªæ…¢å¯ä»¥ `Ctrl + C` é€€å‡ºåæ¢æˆ `pip install -e '.[all]' -i https://mirrors.aliyun.com/pypi/simple/`

å‡å¦‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­æ²¡æœ‰å‡ºç°ä»»ä½•çš„æŠ¥é”™çš„è¯ï¼Œé‚£ä¹Ÿå°±æ„å‘³ç€æˆ‘ä»¬æˆåŠŸå®‰è£…å¥½æ”¯æŒ XTuner æ‰€è¿è¡Œçš„ç¯å¢ƒå•¦ã€‚å…¶å®å¯¹äºå¾ˆå¤šçš„åˆå­¦è€…è€Œè¨€ï¼Œå®‰è£…å¥½ç¯å¢ƒæ„å‘³ç€æˆåŠŸäº†ä¸€å¤§åŠï¼å› æ­¤æˆ‘ä»¬æ¥ä¸‹æ¥å°±å¯ä»¥è¿›å…¥æˆ‘ä»¬çš„ç¬¬äºŒæ­¥ï¼Œå‡†å¤‡å¥½æˆ‘ä»¬éœ€è¦çš„æ•°æ®é›†ã€æ¨¡å‹å’Œé…ç½®æ–‡ä»¶ï¼


#### 2.2.1 æ•°æ®é›†å‡†å¤‡

ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿè®©æ¨¡å‹è®¤æ¸…è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼ŒçŸ¥é“åœ¨è¯¢é—®è‡ªå·±æ˜¯è°çš„æ—¶å€™å›å¤æˆæˆ‘ä»¬æƒ³è¦çš„æ ·å­ï¼Œæˆ‘ä»¬å°±éœ€è¦é€šè¿‡åœ¨å¾®è°ƒæ•°æ®é›†ä¸­å¤§é‡æºæ‚è¿™éƒ¨åˆ†çš„æ•°æ®ã€‚

é¦–å…ˆæˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥å­˜æ”¾æˆ‘ä»¬è¿™æ¬¡è®­ç»ƒæ‰€éœ€è¦çš„æ‰€æœ‰æ–‡ä»¶ã€‚

```bash
# å‰åŠéƒ¨åˆ†æ˜¯åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒååŠéƒ¨åˆ†æ˜¯è¿›å…¥è¯¥æ–‡ä»¶å¤¹ã€‚
mkdir -p /root/ft && cd /root/ft

# åœ¨ftè¿™ä¸ªæ–‡ä»¶å¤¹é‡Œå†åˆ›å»ºä¸€ä¸ªå­˜æ”¾æ•°æ®çš„dataæ–‡ä»¶å¤¹
mkdir -p /root/ft/data && cd /root/ft/data
```



> é™¤äº†æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬çš„æ•°æ®é›†ï¼Œå…¶å®ç½‘ä¸Šä¹Ÿæœ‰å¤§é‡çš„å¼€æºæ•°æ®é›†å¯ä»¥ä¾›æˆ‘ä»¬è¿›è¡Œä½¿ç”¨ã€‚æœ‰äº›æ—¶å€™æˆ‘ä»¬å¯ä»¥åœ¨å¼€æºæ•°æ®é›†çš„åŸºç¡€ä¸Šæ·»åŠ ä¸€äº›æˆ‘ä»¬è‡ªå·±ç‹¬æœ‰çš„æ•°æ®é›†ï¼Œä¹Ÿå¯èƒ½ä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚

#### 2.2.2 æ¨¡å‹å‡†å¤‡

åœ¨å‡†å¤‡å¥½äº†æ•°æ®é›†åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°±éœ€è¦å‡†å¤‡å¥½æˆ‘ä»¬çš„è¦ç”¨äºå¾®è°ƒçš„æ¨¡å‹ã€‚ç”±äºæœ¬æ¬¡è¯¾ç¨‹æ˜¾å­˜æ–¹é¢çš„é™åˆ¶ï¼Œè¿™é‡Œæˆ‘ä»¬å°±ä½¿ç”¨ InternLM æœ€æ–°æ¨å‡ºçš„å°æ¨¡å‹ `InterLM2-Chat-1.8B` æ¥å®Œæˆæ­¤æ¬¡çš„å¾®è°ƒæ¼”ç¤ºã€‚

å¯¹äºåœ¨ InternStudio ä¸Šè¿è¡Œçš„å°ä¼™ä¼´ä»¬ï¼Œå¯ä»¥ä¸ç”¨é€šè¿‡ OpenXLab æˆ–è€… Modelscope è¿›è¡Œæ¨¡å‹çš„ä¸‹è½½ã€‚æˆ‘ä»¬ç›´æ¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®åˆ›å»ºæ–‡ä»¶å¤¹å¹¶å°†æ‰€æœ‰æ–‡ä»¶å¤åˆ¶è¿›å»ã€‚

``` bash
# åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹ï¼Œç¡®ä¿å®ƒå­˜åœ¨ã€‚
# -pé€‰é¡¹æ„å‘³ç€å¦‚æœä¸Šçº§ç›®å½•ä¸å­˜åœ¨ä¹Ÿä¼šä¸€å¹¶åˆ›å»ºï¼Œä¸”å¦‚æœç›®æ ‡æ–‡ä»¶å¤¹å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ¥é”™ã€‚
mkdir -p /root/ft/model

# å¤åˆ¶å†…å®¹åˆ°ç›®æ ‡æ–‡ä»¶å¤¹ã€‚-ré€‰é¡¹è¡¨ç¤ºé€’å½’å¤åˆ¶æ•´ä¸ªæ–‡ä»¶å¤¹ã€‚
cp -r /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/* /root/ft/model/
```
é‚£è¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ°åœ¨ model æ–‡ä»¶å¤¹ä¸‹ä¿å­˜äº†æ¨¡å‹çš„ç›¸å…³æ–‡ä»¶å’Œå†…å®¹äº†ã€‚
```
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- special_tokens_map.json
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```
å‡å¦‚å¤§å®¶å­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ä¸€é”®é€šè¿‡ç¬¦å·é“¾æ¥çš„æ–¹å¼é“¾æ¥åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·æ—¢èŠ‚çœäº†ç©ºé—´ï¼Œä¹Ÿä¾¿äºç®¡ç†ã€‚

```bash
# åˆ é™¤/root/ft/modelç›®å½•
rm -rf /root/ft/model

# åˆ›å»ºç¬¦å·é“¾æ¥
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b /root/ft/model
```
æ‰§è¡Œä¸Šè¿°æ“ä½œåï¼Œ`/root/ft/model` å°†ç›´æ¥æˆä¸ºä¸€ä¸ªç¬¦å·é“¾æ¥ï¼Œè¿™ä¸ªé“¾æ¥æŒ‡å‘ `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` çš„ä½ç½®ã€‚

è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬è®¿é—® `/root/ft/model` æ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨è®¿é—® `/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b` ç›®å½•ä¸‹çš„å†…å®¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬æ— éœ€å¤åˆ¶ä»»ä½•æ•°æ®ï¼Œå°±å¯ä»¥ç›´æ¥åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œåç»­çš„å¾®è°ƒæ“ä½œï¼Œä»è€ŒèŠ‚çœå­˜å‚¨ç©ºé—´å¹¶ç®€åŒ–æ–‡ä»¶ç®¡ç†ã€‚

åœ¨è¯¥æƒ…å†µä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼Œå¯ä»¥çœ‹åˆ°å’Œä¸Šé¢çš„åŒºåˆ«åœ¨äºå¤šäº†ä¸€äº›è½¯é“¾æ¥ç›¸å…³çš„æ–‡ä»¶ã€‚
```
|-- model/
    |-- tokenizer.model
    |-- config.json
    |-- .mdl
    |-- tokenization_internlm2.py
    |-- model-00002-of-00002.safetensors
    |-- tokenizer_config.json
    |-- model-00001-of-00002.safetensors
    |-- model.safetensors.index.json
    |-- configuration.json
    |-- .msc
    |-- special_tokens_map.json
    |-- .mv
    |-- modeling_internlm2.py
    |-- README.md
    |-- configuration_internlm2.py
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

#### 2.2.3 é…ç½®æ–‡ä»¶é€‰æ‹©
åœ¨å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°±è¦æ ¹æ®æˆ‘ä»¬é€‰æ‹©çš„å¾®è°ƒæ–¹æ³•æ–¹æ³•ç»“åˆå‰é¢çš„ä¿¡æ¯æ¥æ‰¾åˆ°ä¸æˆ‘ä»¬æœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶äº†ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬å¯¹é…ç½®æ–‡ä»¶çš„ä¿®æ”¹é‡ã€‚

æ‰€è°“é…ç½®æ–‡ä»¶ï¼ˆconfigï¼‰ï¼Œå…¶å®æ˜¯ä¸€ç§ç”¨äºå®šä¹‰å’Œæ§åˆ¶æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­å„ä¸ªæ–¹é¢çš„å‚æ•°å’Œè®¾ç½®çš„å·¥å…·ã€‚å‡†å¤‡å¥½çš„é…ç½®æ–‡ä»¶åªè¦è¿è¡Œèµ·æ¥å°±ä»£è¡¨ç€æ¨¡å‹å°±å¼€å§‹è®­ç»ƒæˆ–è€…å¾®è°ƒäº†ã€‚

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š
> å¼€ç®±å³ç”¨æ„å‘³ç€å‡å¦‚èƒ½å¤Ÿè¿æ¥ä¸Š Huggingface ä»¥åŠæœ‰è¶³å¤Ÿçš„æ˜¾å­˜ï¼Œå…¶å®å°±å¯ä»¥ç›´æ¥è¿è¡Œè¿™äº›é…ç½®æ–‡ä»¶ï¼ŒXTunerå°±èƒ½å¤Ÿç›´æ¥ä¸‹è½½å¥½è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ç„¶åå¼€å§‹è¿›è¡Œå¾®è°ƒ
```Bash
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®æ–‡ä»¶
# xtuner list-cfg

# å‡å¦‚æˆ‘ä»¬æƒ³æ‰¾åˆ° internlm2-1.8b æ¨¡å‹é‡Œæ”¯æŒçš„é…ç½®æ–‡ä»¶
xtuner list-cfg -p internlm2_1_8b
```
> è¿™é‡Œå°±ç”¨åˆ°äº†ç¬¬ä¸€ä¸ª XTuner çš„å·¥å…· `list-cfg` ï¼Œå¯¹äºè¿™ä¸ªå·¥å…·è€Œè¨€ï¼Œå¯ä»¥é€‰æ‹©ä¸æ·»åŠ é¢å¤–çš„å‚æ•°ï¼Œå°±åƒä¸Šé¢çš„ä¸€æ ·ï¼Œè¿™æ ·å°±ä¼šå°†æ‰€æœ‰çš„é…ç½®æ–‡ä»¶éƒ½æ‰“å°å‡ºæ¥ã€‚é‚£åŒæ—¶ä¹Ÿå¯ä»¥åŠ ä¸Šä¸€ä¸ªå‚æ•° `-p` æˆ– `--pattern` ï¼Œåé¢è¾“å…¥çš„å†…å®¹å°†ä¼šåœ¨æ‰€æœ‰çš„ config æ–‡ä»¶é‡Œè¿›è¡Œæ¨¡ç³ŠåŒ¹é…æœç´¢ï¼Œç„¶åè¿”å›æœ€æœ‰å¯èƒ½å¾—å†…å®¹ã€‚æˆ‘ä»¬å¯ä»¥ç”¨æ¥æœç´¢ç‰¹å®šæ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼Œæ¯”å¦‚ä¾‹å­ä¸­çš„ internlm2_1_8b ,ä¹Ÿå¯ä»¥ç”¨æ¥æœç´¢åƒæ˜¯å¾®è°ƒæ–¹æ³• qlora ã€‚
æ ¹æ®ä¸Šé¢çš„å®šå‘æœç´¢æŒ‡ä»¤å¯ä»¥çœ‹åˆ°ç›®å‰åªæœ‰ä¸¤ä¸ªæ”¯æŒ internlm2-1.8B çš„æ¨¡å‹é…ç½®æ–‡ä»¶ã€‚
```
==========================CONFIGS===========================
PATTERN: internlm2_1_8b
-------------------------------
internlm2_1_8b_full_alpaca_e3
internlm2_1_8b_qlora_alpaca_e3
=============================================================
```
<details>
<summary>é…ç½®æ–‡ä»¶åçš„è§£é‡Š</summary>

ä»¥ **internlm2_1_8b_qlora_alpaca_e3** ä¸¾ä¾‹ï¼š

| æ¨¡å‹å   | è¯´æ˜          |
| -------- | ------------- |
| internlm2_1_8b | æ¨¡å‹åç§° |
| qlora    | ä½¿ç”¨çš„ç®—æ³•     |
| alpaca   | æ•°æ®é›†åç§°     |
| e3       | æŠŠæ•°æ®é›†è·‘3æ¬¡  |

</details>

è™½ç„¶æˆ‘ä»¬ç”¨çš„æ•°æ®é›†å¹¶ä¸æ˜¯ `alpaca` è€Œæ˜¯æˆ‘ä»¬è‡ªå·±é€šè¿‡è„šæœ¬åˆ¶ä½œçš„å°åŠ©æ‰‹æ•°æ®é›† ï¼Œä½†æ˜¯ç”±äºæˆ‘ä»¬æ˜¯é€šè¿‡ `QLoRA` çš„æ–¹å¼å¯¹ `internlm2-chat-1.8b` è¿›è¡Œå¾®è°ƒã€‚è€Œæœ€ç›¸è¿‘çš„é…ç½®æ–‡ä»¶åº”è¯¥å°±æ˜¯ `internlm2_1_8b_qlora_alpaca_e3` ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ‹·è´è¿™ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š
```Bash
# åˆ›å»ºä¸€ä¸ªå­˜æ”¾ config æ–‡ä»¶çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/config

# ä½¿ç”¨ XTuner ä¸­çš„ copy-cfg åŠŸèƒ½å°† config æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„ä½ç½®
xtuner copy-cfg internlm2_1_8b_qlora_alpaca_e3 /root/ft/config
```
> è¿™é‡Œæˆ‘ä»¬å°±ç”¨åˆ°äº† XTuner å·¥å…·ç®±ä¸­çš„ç¬¬äºŒä¸ªå·¥å…· `copy-cfg` ï¼Œè¯¥å·¥å…·æœ‰ä¸¤ä¸ªå¿…é¡»è¦å¡«å†™çš„å‚æ•° `{CONFIG_NAME}` å’Œ `{SAVE_PATH}` ï¼Œåœ¨æˆ‘ä»¬çš„è¾“å…¥çš„è¿™ä¸ªæŒ‡ä»¤ä¸­ï¼Œæˆ‘ä»¬çš„ `{CONFIG_NAME}` å¯¹åº”çš„æ˜¯ä¸Šé¢æœç´¢åˆ°çš„ `internlm2_1_8b_qlora_alpaca_e3` ,è€Œ `{SAVE_PATH}` åˆ™å¯¹åº”çš„æ˜¯åˆšåˆšæ–°å»ºçš„ `/root/ft/config`ã€‚æˆ‘ä»¬å‡å¦‚éœ€è¦å¤åˆ¶å…¶ä»–çš„é…ç½®æ–‡ä»¶åªéœ€è¦ä¿®æ”¹è¿™ä¸¤ä¸ªå‚æ•°å³å¯å®ç°ã€‚
è¾“å…¥åæˆ‘ä»¬å°±èƒ½å¤Ÿçœ‹åˆ°åœ¨æˆ‘ä»¬çš„ `/root/ft/config` æ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€ä¸ªåä¸º `internlm2_1_8b_qlora_alpaca_e3_copy.py` çš„æ–‡ä»¶äº†ã€‚
```
|-- config/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
```
#### 2.2.4 å°ç»“
å®Œæˆä»¥ä¸Šå†…å®¹åï¼Œæˆ‘å°±å·²ç»å®Œæˆäº†æ‰€æœ‰çš„å‡†å¤‡å·¥ä½œäº†ã€‚æˆ‘ä»¬å†æ¥å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åšäº†å“ªäº›äº‹æƒ…ï¼š
1. æˆ‘ä»¬é¦–å…ˆæ˜¯åœ¨ GitHub ä¸Šå…‹éš†äº† XTuner çš„æºç ï¼Œå¹¶æŠŠç›¸å…³çš„é…å¥—åº“ä¹Ÿé€šè¿‡ pip çš„æ–¹å¼è¿›è¡Œäº†å®‰è£…ã€‚
2. ç„¶åæˆ‘ä»¬æ ¹æ®è‡ªå·±æƒ³è¦åšçš„äº‹æƒ…ï¼Œåˆ©ç”¨è„šæœ¬å‡†å¤‡å¥½äº†ä¸€ä»½å…³äºè°ƒæ•™æ¨¡å‹è®¤è¯†è‡ªå·±èº«ä»½å¼Ÿä½çš„æ•°æ®é›†ã€‚
3. å†ç„¶åæˆ‘ä»¬æ ¹æ®è‡ªå·±çš„æ˜¾å­˜åŠä»»åŠ¡æƒ…å†µç¡®å®šäº†ä½¿ç”¨ InternLM2-chat-1.8B è¿™ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”å°†å…¶å¤åˆ¶åˆ°æˆ‘ä»¬çš„æ–‡ä»¶å¤¹é‡Œã€‚
4. æœ€åæˆ‘ä»¬åœ¨ XTuner å·²æœ‰çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œæ ¹æ®å¾®è°ƒæ–¹æ³•ã€æ•°æ®é›†å’Œæ¨¡å‹æŒ‘é€‰å‡ºæœ€åˆé€‚çš„é…ç½®æ–‡ä»¶å¹¶å¤åˆ¶åˆ°æˆ‘ä»¬æ–°å»ºçš„æ–‡ä»¶å¤¹ä¸­ã€‚

ç»è¿‡äº†ä»¥ä¸Šçš„æ­¥éª¤åï¼Œæˆ‘ä»¬çš„ `ft` æ–‡ä»¶å¤¹é‡Œåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
```
|-- ft/
    |-- config/
        |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- model/
        |-- tokenizer.model
        |-- config.json
        |-- tokenization_internlm2.py
        |-- model-00002-of-00002.safetensors
        |-- tokenizer_config.json
        |-- model-00001-of-00002.safetensors
        |-- model.safetensors.index.json
        |-- configuration.json
        |-- special_tokens_map.json
        |-- modeling_internlm2.py
        |-- README.md
        |-- configuration_internlm2.py
        |-- generation_config.json
        |-- tokenization_internlm2_fast.py
    |-- data/
        |-- personal_assistant.json
        |-- generate_data.py
```


å¾®è°ƒä¹Ÿç»å¸¸è¢«æˆç§°ä¸ºæ˜¯ç‚¼ä¸¹ï¼Œå°±æ˜¯è¯´ä½ ç‚¼ä¸¹çš„æ—¶å€™ä½ å¾—æ€è€ƒå¥½ç”¨ä»€ä¹ˆæ ·çš„ææ–™ã€ç”¨å¤šå¤§çš„ç«å€™ã€çƒ¤å¤šä¹…çš„æ—¶é—´ä»¥åŠç”¨ä»€ä¹ˆä¸¹ç‚‰å»çƒ§ã€‚è¿™é‡Œçš„ä¸¹ç‚‰å…¶å®æˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸º XTuner ï¼Œåªè¦ä¸¹ç‚‰çš„è´¨é‡è¿‡å¾—å»ï¼Œç‚¼ä¸¹çš„æ—¶å€™ä¸ä¼šç‚¸ï¼Œä¸€èˆ¬éƒ½æ˜¯æ²¡é—®é¢˜çš„ã€‚ä½†æ˜¯å‡å¦‚ç‚¼ä¸¹çš„ææ–™ï¼ˆå°±æ˜¯æ•°æ®é›†ï¼‰æœ¬æ¥å°±æ˜¯åƒåœ¾ï¼Œé‚£æ— è®ºæ€ä¹ˆç‚¼ï¼ˆå¾®è°ƒå‚æ•°çš„è°ƒæ•´ï¼‰ï¼Œç‚¼å¤šä¹…ï¼ˆè®­ç»ƒçš„è½®æ•°ï¼‰ï¼Œç‚¼å‡ºæ¥çš„ä¸œè¥¿è¿˜åªèƒ½ä¸”åªä¼šæ˜¯åƒåœ¾ã€‚åªæœ‰è¯´ç”¨äº†æ¯”è¾ƒå¥½çš„ææ–™ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥è€ƒè™‘è¯´è¦ç‚¼å¤šä¹…ä»¥åŠç”¨ä»€ä¹ˆåŠæ³•å»ç‚¼çš„é—®é¢˜ã€‚å› æ­¤æ€»çš„æ¥è¯´ï¼Œå­¦ä¼šå¦‚ä½•æ„å»ºä¸€ä»½é«˜è´¨é‡çš„æ•°æ®é›†æ˜¯è‡³å…³é‡è¦çš„ã€‚

### 2.3 é…ç½®æ–‡ä»¶ä¿®æ”¹
åœ¨é€‰æ‹©äº†ä¸€ä¸ªæœ€åŒ¹é…çš„é…ç½®æ–‡ä»¶å¹¶å‡†å¤‡å¥½å…¶ä»–å†…å®¹åï¼Œä¸‹é¢æˆ‘ä»¬è¦åšçš„äº‹æƒ…å°±æ˜¯æ ¹æ®æˆ‘ä»¬è‡ªå·±çš„å†…å®¹å¯¹è¯¥é…ç½®æ–‡ä»¶è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬å®é™…è®­ç»ƒçš„è¦æ±‚ã€‚

<details>
<summary><b>é…ç½®æ–‡ä»¶ä»‹ç»</b></summary>
 
å‡å¦‚æˆ‘ä»¬çœŸçš„æ‰“å¼€é…ç½®æ–‡ä»¶åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•´ä½“çš„é…ç½®æ–‡ä»¶åˆ†ä¸ºäº”éƒ¨åˆ†ï¼š
1. **PART 1 Settings**ï¼šæ¶µç›–äº†æ¨¡å‹åŸºæœ¬è®¾ç½®ï¼Œå¦‚é¢„è®­ç»ƒæ¨¡å‹çš„é€‰æ‹©ã€æ•°æ®é›†ä¿¡æ¯å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›åŸºæœ¬å‚æ•°ï¼ˆå¦‚æ‰¹å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼‰ã€‚

2. **PART 2 Model & Tokenizer**ï¼šæŒ‡å®šäº†ç”¨äºè®­ç»ƒçš„æ¨¡å‹å’Œåˆ†è¯å™¨çš„å…·ä½“ç±»å‹åŠå…¶é…ç½®ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„å’Œæ˜¯å¦å¯ç”¨ç‰¹å®šåŠŸèƒ½ï¼ˆå¦‚å¯å˜é•¿åº¦æ³¨æ„åŠ›ï¼‰ï¼Œè¿™æ˜¯æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚

3. **PART 3 Dataset & Dataloader**ï¼šæè¿°äº†æ•°æ®å¤„ç†çš„ç»†èŠ‚ï¼ŒåŒ…æ‹¬å¦‚ä½•åŠ è½½æ•°æ®é›†ã€é¢„å¤„ç†æ­¥éª¤ã€æ‰¹å¤„ç†å¤§å°ç­‰ï¼Œç¡®ä¿äº†æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶åˆ°æ­£ç¡®æ ¼å¼å’Œè´¨é‡çš„æ•°æ®ã€‚

4. **PART 4 Scheduler & Optimizer**ï¼šé…ç½®äº†ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®å‚æ•°ï¼Œå¦‚å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å’Œä¼˜åŒ–å™¨çš„é€‰æ‹©ï¼Œè¿™äº›æ˜¯å½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœå’Œé€Ÿåº¦çš„é‡è¦å› ç´ ã€‚

5. **PART 5 Runtime**ï¼šå®šä¹‰äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢å¤–è®¾ç½®ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¨¡å‹ä¿å­˜ç­–ç•¥å’Œè‡ªå®šä¹‰é’©å­ç­‰ï¼Œä»¥æ”¯æŒè®­ç»ƒæµç¨‹çš„ç›‘æ§ã€è°ƒè¯•å’Œç»“æœçš„ä¿å­˜ã€‚

ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬éœ€è¦æ›´æ”¹çš„éƒ¨åˆ†å…¶å®åªåŒ…æ‹¬å‰ä¸‰éƒ¨åˆ†ï¼Œè€Œä¸”ä¿®æ”¹çš„ä¸»è¦åŸå› æ˜¯æˆ‘ä»¬ä¿®æ”¹äº†é…ç½®æ–‡ä»¶ä¸­è§„å®šçš„æ¨¡å‹ã€æ•°æ®é›†ã€‚åä¸¤éƒ¨åˆ†éƒ½æ˜¯ XTuner å®˜æ–¹å¸®æˆ‘ä»¬ä¼˜åŒ–å¥½çš„ä¸œè¥¿ï¼Œä¸€èˆ¬è€Œè¨€åªæœ‰åœ¨é­”æ”¹çš„æƒ…å†µä¸‹æ‰éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚ä¸‹é¢æˆ‘ä»¬å°†æ ¹æ®é¡¹ç›®çš„è¦æ±‚ä¸€æ­¥æ­¥çš„è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´å§ï¼
</details>

é€šè¿‡æŠ˜å éƒ¨åˆ†çš„ä¿®æ”¹ï¼Œå†…å®¹å¦‚ä¸‹ï¼Œå¯ä»¥ç›´æ¥å°†ä»¥ä¸‹ä»£ç å¤åˆ¶åˆ°Â `/root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py`Â æ–‡ä»¶ä¸­ï¼ˆå…ˆ `Ctrl + A` é€‰ä¸­æ‰€æœ‰æ–‡ä»¶å¹¶åˆ é™¤åå†å°†ä»£ç å¤åˆ¶è¿›å»ï¼‰ã€‚
<details>
<summary><b>å‚æ•°ä¿®æ”¹ç»†èŠ‚</b></summary>

é¦–å…ˆåœ¨ PART 1 çš„éƒ¨åˆ†ï¼Œç”±äºæˆ‘ä»¬ä¸å†éœ€è¦åœ¨ Huggingface ä¸Šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å…ˆè¦æ›´æ¢æ¨¡å‹çš„è·¯å¾„ä»¥åŠæ•°æ®é›†çš„è·¯å¾„ä¸ºæˆ‘ä»¬æœ¬åœ°çš„è·¯å¾„ã€‚
    
```diff
# ä¿®æ”¹æ¨¡å‹åœ°å€ï¼ˆåœ¨ç¬¬27è¡Œçš„ä½ç½®ï¼‰
- pretrained_model_name_or_path = 'internlm/internlm2-1_8b'
+ pretrained_model_name_or_path = '/root/ft/model'

# ä¿®æ”¹æ•°æ®é›†åœ°å€ä¸ºæœ¬åœ°çš„jsonæ–‡ä»¶åœ°å€ï¼ˆåœ¨ç¬¬31è¡Œçš„ä½ç½®ï¼‰
- alpaca_en_path = 'tatsu-lab/alpaca'
+ alpaca_en_path = '/root/ft/data/personal_assistant.json'
```

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¯¹ä¸€äº›é‡è¦çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ï¼ˆlrï¼‰ã€è®­ç»ƒçš„è½®æ•°ï¼ˆmax_epochsï¼‰ç­‰ç­‰ã€‚ç”±äºæˆ‘ä»¬è¿™æ¬¡åªæ˜¯ä¸€ä¸ªç®€å•çš„è®©æ¨¡å‹çŸ¥é“è‡ªå·±çš„èº«ä»½å¼Ÿä½ï¼Œå› æ­¤æˆ‘ä»¬çš„è®­ç»ƒè½®æ•°ä»¥åŠå•æ¡æ•°æ®æœ€å¤§çš„ Token æ•°ï¼ˆmax_lengthï¼‰éƒ½å¯ä»¥ä¸ç”¨é‚£ä¹ˆå¤§ã€‚

```diff
# ä¿®æ”¹max_lengthæ¥é™ä½æ˜¾å­˜çš„æ¶ˆè€—ï¼ˆåœ¨ç¬¬33è¡Œçš„ä½ç½®ï¼‰
- max_length = 2048
+ max_length = 1024

# å‡å°‘è®­ç»ƒçš„è½®æ•°ï¼ˆåœ¨ç¬¬44è¡Œçš„ä½ç½®ï¼‰
- max_epochs = 3
+ max_epochs = 2

# å¢åŠ ä¿å­˜æƒé‡æ–‡ä»¶çš„æ€»æ•°ï¼ˆåœ¨ç¬¬54è¡Œçš„ä½ç½®ï¼‰
- save_total_limit = 2
+ save_total_limit = 3
```

å¦å¤–ï¼Œä¸ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿåˆ°æ¨¡å‹çš„å˜åŒ–æƒ…å†µï¼ŒXTuner ä¹Ÿæ˜¯è´´å¿ƒçš„æ¨å‡ºäº†ä¸€ä¸ª `evaluation_inputs` çš„å‚æ•°æ¥è®©æˆ‘ä»¬èƒ½å¤Ÿè®¾ç½®å¤šä¸ªé—®é¢˜æ¥ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯æœç€æˆ‘ä»¬æƒ³è¦çš„æ–¹å‘å‰è¿›çš„ã€‚æ¯”å¦‚è¯´æˆ‘ä»¬è¿™é‡Œæ˜¯å¸Œæœ›åœ¨é—®å‡º â€œè¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±â€ æˆ–è€…è¯´ â€œä½ æ˜¯è°â€ çš„æ—¶å€™ï¼Œæ¨¡å‹èƒ½å¤Ÿç»™ä½ çš„å›å¤æ˜¯ â€œæˆ‘æ˜¯XXXçš„å°åŠ©æ‰‹...â€ è¿™æ ·çš„å›å¤ã€‚å› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥æ ¹æ®è¿™ä¸ªéœ€æ±‚è¿›è¡Œæ›´æ”¹ã€‚


``` diff
# ä¿®æ”¹æ¯å¤šå°‘è½®è¿›è¡Œä¸€æ¬¡è¯„ä¼°ï¼ˆåœ¨ç¬¬57è¡Œçš„ä½ç½®ï¼‰
- evaluation_freq = 500
+ evaluation_freq = 300

# ä¿®æ”¹å…·ä½“è¯„ä¼°çš„é—®é¢˜ï¼ˆåœ¨ç¬¬59åˆ°61è¡Œçš„ä½ç½®ï¼‰
# å¯ä»¥è‡ªç”±æ‹“å±•å…¶ä»–é—®é¢˜
- evaluation_inputs = ['è¯·ç»™æˆ‘ä»‹ç»äº”ä¸ªä¸Šæµ·çš„æ™¯ç‚¹', 'Please tell me five scenic spots in Shanghai']
+ evaluation_inputs = ['è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ æ˜¯è°', 'ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—']
```
è¿™æ ·ä¿®æ”¹å®Œååœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å°±ä¼šæ˜¾ç¤ºåœ¨å½“å‰çš„æƒé‡æ–‡ä»¶ä¸‹æ¨¡å‹å¯¹è¿™å‡ ä¸ªé—®é¢˜çš„å›å¤äº†ã€‚

ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†ä¸å†æ˜¯åŸæœ¬çš„ aplaca æ•°æ®é›†ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿè¦è¿›å…¥ PART 3 çš„éƒ¨åˆ†å¯¹ç›¸å…³çš„å†…å®¹è¿›è¡Œä¿®æ”¹ã€‚åŒ…æ‹¬è¯´æˆ‘ä»¬æ•°æ®é›†è¾“å…¥çš„ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹è€Œæ˜¯ä¸€ä¸ªå•çº¯çš„ json æ–‡ä»¶ä»¥åŠæˆ‘ä»¬çš„æ•°æ®é›†æ ¼å¼è¦æ±‚æ”¹ä¸ºæˆ‘ä»¬æœ€é€šç”¨çš„ OpenAI æ•°æ®é›†æ ¼å¼ã€‚

``` diff
# æŠŠ OpenAI æ ¼å¼çš„ map_fn è½½å…¥è¿›æ¥ï¼ˆåœ¨ç¬¬15è¡Œçš„ä½ç½®ï¼‰
- from xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import openai_map_fn, template_map_fn_factory

# å°†åŸæœ¬æ˜¯ alpaca çš„åœ°å€æ”¹ä¸ºæ˜¯ json æ–‡ä»¶çš„åœ°å€ï¼ˆåœ¨ç¬¬102è¡Œçš„ä½ç½®ï¼‰
- dataset=dict(type=load_dataset, path=alpaca_en_path),
+ dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),

# å°† dataset_map_fn æ”¹ä¸ºé€šç”¨çš„ OpenAI æ•°æ®é›†æ ¼å¼ï¼ˆåœ¨ç¬¬105è¡Œçš„ä½ç½®ï¼‰
- dataset_map_fn=alpaca_map_fn,
+ dataset_map_fn=openai_map_fn,
```

<details>
<summary><b>å¸¸ç”¨å‚æ•°ä»‹ç»</b></summary>

**å¸¸ç”¨è¶…å‚**

| å‚æ•°å                  | è§£é‡Š                                                     |
| ----------------------- | -------------------------------------------------------- |
| **data_path**           | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                             |
| **max_length**          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                         |
| **pack_to_max_length**  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡        |
| **accumulative_counts** | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°                  |
| **sequence_parallel_size** | å¹¶è¡Œåºåˆ—å¤„ç†çš„å¤§å°ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„åºåˆ—å¹¶è¡Œ              |
| **batch_size**          | æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ‰¹é‡å¤§å°                                      |
| **dataloader_num_workers** | æ•°æ®åŠ è½½å™¨ä¸­å·¥ä½œè¿›ç¨‹çš„æ•°é‡                                |
| **max_epochs**          | è®­ç»ƒçš„æœ€å¤§è½®æ•°                                             |
| **optim_type**          | ä¼˜åŒ–å™¨ç±»å‹ï¼Œä¾‹å¦‚ AdamW                                    |
| **lr**                  | å­¦ä¹ ç‡                                                    |
| **betas**               | ä¼˜åŒ–å™¨ä¸­çš„ beta å‚æ•°ï¼Œæ§åˆ¶åŠ¨é‡å’Œå¹³æ–¹æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡         |
| **weight_decay**        | æƒé‡è¡°å‡ç³»æ•°ï¼Œç”¨äºæ­£åˆ™åŒ–å’Œé¿å…è¿‡æ‹Ÿåˆ                      |
| **max_norm**            | æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°ï¼Œç”¨äºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                      |
| **warmup_ratio**        | é¢„çƒ­çš„æ¯”ä¾‹ï¼Œå­¦ä¹ ç‡åœ¨è¿™ä¸ªæ¯”ä¾‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§å¢åŠ åˆ°åˆå§‹å­¦ä¹ ç‡ |
| **save_steps**          | ä¿å­˜æ¨¡å‹çš„æ­¥æ•°é—´éš”                                         |
| **save_total_limit**    | ä¿å­˜çš„æ¨¡å‹æ€»æ•°é™åˆ¶ï¼Œè¶…è¿‡é™åˆ¶æ—¶åˆ é™¤æ—§çš„æ¨¡å‹æ–‡ä»¶             |
| **prompt_template**     | æ¨¡æ¿æç¤ºï¼Œç”¨äºå®šä¹‰ç”Ÿæˆæ–‡æœ¬çš„æ ¼å¼æˆ–ç»“æ„                    |
| ...... | ...... |

> å¦‚æœæƒ³æŠŠæ˜¾å¡çš„ç°å­˜åƒæ»¡ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¡èµ„æºï¼Œå¯ä»¥å°† `max_length` å’Œ `batch_size` è¿™ä¸¤ä¸ªå‚æ•°è°ƒå¤§ã€‚
</details>


</details>

```python
# Copyright (c) OpenMMLab. All rights reserved.
import torch
from datasets import load_dataset
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR
from peft import LoraConfig
from torch.optim import AdamW
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                          BitsAndBytesConfig)

from xtuner.dataset import process_hf_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import openai_map_fn, template_map_fn_factory
from xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,
                                 VarlenAttnArgsToMessageHubHook)
from xtuner.engine.runner import TrainLoop
from xtuner.model import SupervisedFinetune
from xtuner.parallel.sequence import SequenceParallelSampler
from xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE

#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = '/root/ft/model'
use_varlen_attn = False

# Data
alpaca_en_path = '/root/ft/data/personal_assistant.json'
prompt_template = PROMPT_TEMPLATE.internlm2_chat
max_length = 1024
pack_to_max_length = True

# parallel
sequence_parallel_size = 1

# Scheduler & Optimizer
batch_size = 1  # per_device
accumulative_counts = 16
accumulative_counts *= sequence_parallel_size
dataloader_num_workers = 0
max_epochs = 2
optim_type = AdamW
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03

# Save
save_steps = 300
save_total_limit = 3  # Maximum checkpoints to keep (-1 means unlimited)

# Evaluate the generation performance during the training
evaluation_freq = 300
SYSTEM = ''
evaluation_inputs = ['è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'ä½ æ˜¯è°', 'ä½ æ˜¯æˆ‘çš„å°åŠ©æ‰‹å—']

#######################################################################
#                      PART 2  Model & Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side='right')

model = dict(
    type=SupervisedFinetune,
    use_varlen_attn=use_varlen_attn,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4')),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias='none',
        task_type='CAUSAL_LM'))

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
alpaca_en = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path='json', data_files=dict(train=alpaca_en_path)),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=openai_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length,
    use_varlen_attn=use_varlen_attn)

sampler = SequenceParallelSampler \
    if sequence_parallel_size > 1 else DefaultSampler
train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=alpaca_en,
    sampler=dict(type=sampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))

#######################################################################
#                    PART 4  Scheduler & Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale='dynamic',
    dtype='float16')

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = [
    dict(
        type=LinearLR,
        start_factor=1e-5,
        by_epoch=True,
        begin=0,
        end=warmup_ratio * max_epochs,
        convert_to_iter_based=True),
    dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        begin=warmup_ratio * max_epochs,
        end=max_epochs,
        convert_to_iter_based=True)
]

# train, val, test setting
train_cfg = dict(type=TrainLoop, max_epochs=max_epochs)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

if use_varlen_attn:
    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 10 iterations.
    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per `save_steps`.
    checkpoint=dict(
        type=CheckpointHook,
        by_epoch=False,
        interval=save_steps,
        max_keep_ckpts=save_total_limit),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend='nccl'),
)

# set visualizer
visualizer = None

# set log level
log_level = 'INFO'

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)

# set log processor
log_processor = dict(by_epoch=False)
```



### 2.4 æ¨¡å‹è®­ç»ƒ

#### 2.4.1 å¸¸è§„è®­ç»ƒ

å½“æˆ‘ä»¬å‡†å¤‡å¥½äº†é…ç½®æ–‡ä»¶å¥½ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä½¿ç”¨ `xtuner train` æŒ‡ä»¤å³å¯å¼€å§‹è®­ç»ƒã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ  `--work-dir` æŒ‡å®šç‰¹å®šçš„æ–‡ä»¶ä¿å­˜ä½ç½®ï¼Œæ¯”å¦‚è¯´å°±ä¿å­˜åœ¨ `/root/ft/train` è·¯å¾„ä¸‹ã€‚å‡å¦‚ä¸æ·»åŠ çš„è¯æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹æ–‡ä»¶å°†é»˜è®¤ä¿å­˜åœ¨ `./work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy` çš„ä½ç½®ï¼Œå°±æ¯”å¦‚è¯´æˆ‘æ˜¯åœ¨ `/root/ft/train` çš„è·¯å¾„ä¸‹è¾“å…¥è¯¥æŒ‡ä»¤ï¼Œé‚£ä¹ˆæˆ‘çš„æ–‡ä»¶ä¿å­˜çš„ä½ç½®å°±æ˜¯åœ¨ `/root/ft/train/work_dirs/internlm2_1_8b_qlora_alpaca_e3_copy` çš„ä½ç½®ä¸‹ã€‚

```bash
# æŒ‡å®šä¿å­˜è·¯å¾„
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train
```
åœ¨è¾“å…¥è®­ç»ƒå®Œåçš„æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š
```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
```

#### 2.4.2 ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç»“åˆ XTuner å†…ç½®çš„ `deepspeed` æ¥åŠ é€Ÿæ•´ä½“çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå…±æœ‰ä¸‰ç§ä¸åŒçš„ `deepspeed` ç±»å‹å¯è¿›è¡Œé€‰æ‹©ï¼Œåˆ†åˆ«æ˜¯ `deepspeed_zero1`, `deepspeed_zero2` å’Œ `deepspeed_zero3`ï¼ˆè¯¦ç»†çš„ä»‹ç»å¯çœ‹ä¸‹æ‹‰æ¡†ï¼‰ã€‚

<details>
<summary>DeepSpeedä¼˜åŒ–å™¨åŠå…¶é€‰æ‹©æ–¹æ³•</summary>

DeepSpeedæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œç”±å¾®è½¯å¼€å‘ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚å®ƒé€šè¿‡å‡ ç§å…³é”®æŠ€æœ¯æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€æ¢¯åº¦ç´¯ç§¯ã€ä»¥åŠå†…å­˜å’Œå¸¦å®½ä¼˜åŒ–ç­‰ã€‚DeepSpeedç‰¹åˆ«é€‚ç”¨äºéœ€è¦å·¨å¤§è®¡ç®—èµ„æºçš„å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†ã€‚

åœ¨DeepSpeedä¸­ï¼Œ`zero` ä»£è¡¨â€œZeROâ€ï¼ˆZero Redundancy Optimizerï¼‰ï¼Œæ˜¯ä¸€ç§æ—¨åœ¨é™ä½è®­ç»ƒå¤§å‹æ¨¡å‹æ‰€éœ€å†…å­˜å ç”¨çš„ä¼˜åŒ–å™¨ã€‚ZeRO é€šè¿‡ä¼˜åŒ–æ•°æ®å¹¶è¡Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œå…è®¸æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚ZeRO åˆ†ä¸ºå‡ ä¸ªä¸åŒçš„çº§åˆ«ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- **deepspeed_zero1**ï¼šè¿™æ˜¯ZeROçš„åŸºæœ¬ç‰ˆæœ¬ï¼Œå®ƒä¼˜åŒ–äº†æ¨¡å‹å‚æ•°çš„å­˜å‚¨ï¼Œä½¿å¾—æ¯ä¸ªGPUåªå­˜å‚¨ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå‡å°‘å†…å­˜çš„ä½¿ç”¨ã€‚

- **deepspeed_zero2**ï¼šåœ¨deepspeed_zero1çš„åŸºç¡€ä¸Šï¼Œdeepspeed_zero2è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜å‚¨ã€‚å®ƒå°†è¿™äº›ä¿¡æ¯ä¹Ÿåˆ†æ•£åˆ°ä¸åŒçš„GPUä¸Šï¼Œè¿›ä¸€æ­¥é™ä½äº†å•ä¸ªGPUçš„å†…å­˜éœ€æ±‚ã€‚

- **deepspeed_zero3**ï¼šè¿™æ˜¯ç›®å‰æœ€é«˜çº§çš„ä¼˜åŒ–ç­‰çº§ï¼Œå®ƒä¸ä»…åŒ…æ‹¬äº†deepspeed_zero1å’Œdeepspeed_zero2çš„ä¼˜åŒ–ï¼Œè¿˜è¿›ä¸€æ­¥å‡å°‘äº†æ¿€æ´»å‡½æ•°çš„å†…å­˜å ç”¨ã€‚è¿™é€šè¿‡åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—æ¿€æ´»ï¼ˆè€Œä¸æ˜¯å­˜å‚¨å®ƒä»¬ï¼‰æ¥å®ç°ï¼Œä»è€Œå®ç°äº†å¯¹å¤§å‹æ¨¡å‹æå…¶å†…å­˜æ•ˆç‡çš„è®­ç»ƒã€‚

é€‰æ‹©å“ªç§deepspeedç±»å‹ä¸»è¦å–å†³äºä½ çš„å…·ä½“éœ€æ±‚ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å¤§å°ã€å¯ç”¨çš„ç¡¬ä»¶èµ„æºï¼ˆç‰¹åˆ«æ˜¯GPUå†…å­˜ï¼‰ä»¥åŠè®­ç»ƒçš„æ•ˆç‡éœ€æ±‚ã€‚ä¸€èˆ¬æ¥è¯´ï¼š

- å¦‚æœä½ çš„æ¨¡å‹è¾ƒå°ï¼Œæˆ–è€…å†…å­˜èµ„æºå……è¶³ï¼Œå¯èƒ½ä¸éœ€è¦ä½¿ç”¨æœ€é«˜çº§åˆ«çš„ä¼˜åŒ–ã€‚
- å¦‚æœä½ æ­£åœ¨å°è¯•è®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ï¼Œæˆ–è€…ä½ çš„ç¡¬ä»¶èµ„æºæœ‰é™ï¼Œä½¿ç”¨deepspeed_zero2æˆ–deepspeed_zero3å¯èƒ½æ›´åˆé€‚ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§æ¨¡å‹çš„è®­ç»ƒã€‚
- é€‰æ‹©æ—¶ä¹Ÿè¦è€ƒè™‘åˆ°å®ç°çš„å¤æ‚æ€§å’Œè¿è¡Œæ—¶çš„å¼€é”€ï¼Œæ›´é«˜çº§çš„ä¼˜åŒ–å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è®¾ç½®ï¼Œå¹¶å¯èƒ½å¢åŠ ä¸€äº›è®¡ç®—å¼€é”€ã€‚

</details>

```bash
# ä½¿ç”¨ deepspeed æ¥åŠ é€Ÿè®­ç»ƒ
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train_deepspeed --deepspeed deepspeed_zero2
```
å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ `deepspeed` æ¥è®­ç»ƒåå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’ŒåŸæœ¬çš„æƒé‡æ–‡ä»¶æ˜¯æœ‰æ‰€å·®åˆ«çš„ï¼ŒåŸæœ¬çš„ä»…ä»…æ˜¯ä¸€ä¸ª .pth çš„æ–‡ä»¶ï¼Œè€Œä½¿ç”¨äº† `deepspeed` åˆ™æ˜¯ä¸€ä¸ªåå­—å¸¦æœ‰ .pth çš„æ–‡ä»¶å¤¹ï¼Œåœ¨è¯¥æ–‡ä»¶å¤¹é‡Œä¿å­˜äº†ä¸¤ä¸ª .pt æ–‡ä»¶ã€‚å½“ç„¶è¿™ä¸¤è€…åœ¨å…·ä½“çš„ä½¿ç”¨ä¸Šå¹¶æ²¡æœ‰å¤ªå¤§çš„å·®åˆ«ï¼Œéƒ½æ˜¯å¯ä»¥è¿›è¡Œè½¬åŒ–å¹¶æ•´åˆã€‚

```
|-- train_deepspeed/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- zero_to_fp32.py
    |-- last_checkpoint
    |-- iter_600.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- 20240406_220727/
        |-- 20240406_220727.log
        |-- vis_data/
            |-- 20240406_220727.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- iter_768.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
    |-- iter_300.pth/
        |-- bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
        |-- mp_rank_00_model_states.pt
```


#### 2.4.3 è®­ç»ƒç»“æœ


å‡å¦‚æˆ‘ä»¬æƒ³è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¶å®å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹å¼è§£å†³ï¼š

1. **å‡å°‘ä¿å­˜æƒé‡æ–‡ä»¶çš„é—´éš”å¹¶å¢åŠ æƒé‡æ–‡ä»¶ä¿å­˜çš„ä¸Šé™**ï¼šè¿™ä¸ªæ–¹æ³•å®é™…ä¸Šå°±æ˜¯é€šè¿‡é™ä½é—´éš”ç»“åˆè¯„ä¼°é—®é¢˜çš„ç»“æœï¼Œä»è€Œæ‰¾åˆ°æœ€ä¼˜çš„æƒé‡æ–‡ã€‚æˆ‘ä»¬å¯ä»¥æ¯éš”100ä¸ªæ‰¹æ¬¡æ¥çœ‹ä»€ä¹ˆæ—¶å€™æ¨¡å‹å·²ç»å­¦åˆ°äº†è¿™éƒ¨åˆ†çŸ¥è¯†ä½†æ˜¯è¿˜ä¿ç•™ç€åŸºæœ¬çš„å¸¸è¯†ï¼Œä»€ä¹ˆæ—¶å€™å·²ç»è¿‡æ‹Ÿåˆä¸¥é‡åªä¼šè¯´ä¸€å¥è¯äº†ã€‚ä½†æ˜¯ç”±äºå†é…ç½®æ–‡ä»¶æœ‰è®¾ç½®æƒé‡æ–‡ä»¶ä¿å­˜æ•°é‡çš„ä¸Šé™ï¼Œå› æ­¤åŒæ—¶å°†è¿™ä¸ªä¸Šé™åŠ å¤§ä¹Ÿæ˜¯éå¸¸å¿…è¦çš„ã€‚
2. **å¢åŠ å¸¸è§„çš„å¯¹è¯æ•°æ®é›†ä»è€Œç¨€é‡ŠåŸæœ¬æ•°æ®çš„å æ¯”**ï¼šè¿™ä¸ªæ–¹æ³•å…¶å®å°±æ˜¯å¸Œæœ›æˆ‘ä»¬æ­£å¸¸ç”¨å¯¹è¯æ•°æ®é›†åšæŒ‡ä»¤å¾®è°ƒçš„åŒæ—¶è¿˜åŠ ä¸Šä¸€éƒ¨åˆ†çš„æ•°æ®é›†æ¥è®©æ¨¡å‹æ—¢èƒ½å¤Ÿå­¦åˆ°æ­£å¸¸å¯¹è¯ï¼Œä½†æ˜¯åœ¨é‡åˆ°ç‰¹å®šé—®é¢˜æ—¶è¿›è¡Œç‰¹æ®ŠåŒ–å¤„ç†ã€‚æ¯”å¦‚è¯´æˆ‘åœ¨ä¸€ä¸‡æ¡æ­£å¸¸çš„å¯¹è¯æ•°æ®é‡Œæ··å…¥ä¸¤åƒæ¡å’Œå°åŠ©æ‰‹ç›¸å…³çš„æ•°æ®é›†ï¼Œè¿™æ ·æ¨¡å‹åŒæ ·å¯ä»¥åœ¨ä¸ä¸¢å¤±å¯¹è¯èƒ½åŠ›çš„å‰æä¸‹å­¦åˆ°å‰‘é”‹å¤§ä½¬çš„å°åŠ©æ‰‹è¿™å¥è¯ã€‚è¿™ç§å…¶å®æ˜¯æ¯”è¾ƒå¸¸è§çš„å¤„ç†æ–¹å¼ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±åŠ¨æ‰‹å°è¯•å®è·µä¸€ä¸‹ã€‚

> å¦å¤–å‡å¦‚æˆ‘ä»¬æ¨¡å‹ä¸­é€”ä¸­æ–­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å‚è€ƒä»¥ä¸‹æ–¹æ³•å®ç°æ¨¡å‹ç»­è®­å·¥ä½œ

<details>
<summary>æ¨¡å‹ç»­è®­æŒ‡å—</summary>

å‡å¦‚æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çªç„¶è¢«ä¸­æ–­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡åœ¨åŸæœ‰æŒ‡ä»¤çš„åŸºç¡€ä¸ŠåŠ ä¸Š `--resume {checkpoint_path}` æ¥å®ç°æ¨¡å‹çš„ç»§ç»­è®­ç»ƒã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªç»§ç»­è®­ç»ƒå¾—åˆ°çš„æƒé‡æ–‡ä»¶å’Œä¸­æ–­å‰çš„å®Œå…¨ä¸€è‡´ï¼Œå¹¶ä¸ä¼šæœ‰ä»»ä½•åŒºåˆ«ã€‚ä¸‹é¢æˆ‘å°†ç”¨è®­ç»ƒäº†500è½®çš„ä¾‹å­æ¥è¿›è¡Œæ¼”ç¤ºã€‚

```bash
# æ¨¡å‹ç»­è®­
xtuner train /root/ft/config/internlm2_1_8b_qlora_alpaca_e3_copy.py --work-dir /root/ft/train --resume /root/ft/train/iter_600.pth
```
åœ¨å®æµ‹è¿‡ç¨‹ä¸­ï¼Œè™½ç„¶æƒé‡æ–‡ä»¶å¹¶æ²¡æœ‰å‘ç”Ÿæ”¹å˜ï¼Œä½†æ˜¯ä¼šå¤šä¸€ä¸ªä»¥æ—¶é—´æˆ³ä¸ºåçš„è®­ç»ƒè¿‡ç¨‹æ–‡ä»¶å¤¹ä¿å­˜è®­ç»ƒçš„è¿‡ç¨‹æ•°æ®ã€‚
```
|-- train/
    |-- internlm2_1_8b_qlora_alpaca_e3_copy.py
    |-- iter_600.pth
    |-- last_checkpoint
    |-- iter_768.pth
    |-- iter_300.pth
    |-- 20240406_203957/
        |-- 20240406_203957.log
        |-- vis_data/
            |-- 20240406_203957.json
            |-- eval_outputs_iter_599.txt
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- eval_outputs_iter_299.txt
            |-- config.py
    |-- 20240406_225723/
        |-- 20240406_225723.log
        |-- vis_data/
            |-- 20240406_225723.json
            |-- eval_outputs_iter_767.txt
            |-- scalars.json
            |-- config.py
```
</details>

#### 2.4.4 å°ç»“
åœ¨æœ¬èŠ‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯è®²è§£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç§ç§ç»†èŠ‚å†…å®¹ï¼ŒåŒ…æ‹¬äº†æ¨¡å‹è®­ç»ƒä¸­çš„å„ä¸ªå‚æ•°ä»¥ã€æƒé‡æ–‡ä»¶çš„é€‰æ‹©æ–¹å¼ä»¥åŠæ¨¡å‹ç»­è®­çš„æ–¹æ³•ã€‚å¯ä»¥çœ‹åˆ°æ˜¯å¦ä½¿ç”¨ `--work-dir` å’Œ æ˜¯å¦ä½¿ç”¨ `--deepspeed` ä¼šå¯¹æ–‡ä»¶çš„ä¿å­˜ä½ç½®ä»¥åŠæƒé‡æ–‡ä»¶çš„ä¿å­˜æ–¹å¼æœ‰æ‰€ä¸åŒï¼Œå¤§å®¶ä¹Ÿå¯ä»¥é€šè¿‡å®è·µå»å®é™…çš„æµ‹è¯•æ„Ÿå—ä¸€ä¸‹ã€‚é‚£ä¹ˆåœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè®­ç»ƒå¾—åˆ°çš„ .pth æ–‡ä»¶è¿›è¡Œä¸‹ä¸€æ­¥çš„è½¬æ¢å’Œæ•´åˆå·¥ä½œäº†ï¼

### 2.5 æ¨¡å‹è½¬æ¢ã€æ•´åˆã€æµ‹è¯•åŠéƒ¨ç½²
#### 2.5.1 æ¨¡å‹è½¬æ¢
æ¨¡å‹è½¬æ¢çš„æœ¬è´¨å…¶å®å°±æ˜¯å°†åŸæœ¬ä½¿ç”¨ Pytorch è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶è½¬æ¢ä¸ºç›®å‰é€šç”¨çš„ Huggingface æ ¼å¼æ–‡ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‡ä»¤æ¥å®ç°ä¸€é”®è½¬æ¢ã€‚

``` bash
# åˆ›å»ºä¸€ä¸ªä¿å­˜è½¬æ¢å Huggingface æ ¼å¼çš„æ–‡ä»¶å¤¹
mkdir -p /root/ft/huggingface

# æ¨¡å‹è½¬æ¢
# xtuner convert pth_to_hf ${é…ç½®æ–‡ä»¶åœ°å€} ${æƒé‡æ–‡ä»¶åœ°å€} ${è½¬æ¢åæ¨¡å‹ä¿å­˜åœ°å€}
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_768.pth /root/ft/huggingface
```
è½¬æ¢å®Œæˆåï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹è¢«è½¬æ¢ä¸º Huggingface ä¸­å¸¸ç”¨çš„ .bin æ ¼å¼æ–‡ä»¶ï¼Œè¿™å°±ä»£è¡¨ç€æ–‡ä»¶æˆåŠŸè¢«è½¬åŒ–ä¸º Huggingface æ ¼å¼äº†ã€‚
```
|-- huggingface/
    |-- adapter_config.json
    |-- xtuner_config.py
    |-- adapter_model.bin
    |-- README.md
```

<span style="color: red;">**æ­¤æ—¶ï¼Œhuggingface æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**</span>

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨è½¬æ¢çš„æŒ‡ä»¤ä¸­æ·»åŠ å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªï¼š
| å‚æ•°å | è§£é‡Š |
| ------------------- | ------------------------------------------------------ |
| --fp32     | ä»£è¡¨ä»¥fp32çš„ç²¾åº¦å¼€å¯ï¼Œå‡å¦‚ä¸è¾“å…¥åˆ™é»˜è®¤ä¸ºfp16                          |
| --max-shard-size {GB}        | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |

å‡å¦‚æœ‰ç‰¹å®šçš„éœ€è¦ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šé¢çš„è½¬æ¢æŒ‡ä»¤åè¿›è¡Œæ·»åŠ ã€‚ç”±äºæœ¬æ¬¡æµ‹è¯•çš„æ¨¡å‹æ–‡ä»¶è¾ƒå°ï¼Œå¹¶ä¸”å·²ç»éªŒè¯è¿‡æ‹Ÿåˆï¼Œæ•…æ²¡æœ‰æ·»åŠ ã€‚å‡å¦‚åŠ ä¸Šçš„è¯åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
```bash
xtuner convert pth_to_hf /root/ft/train/internlm2_1_8b_qlora_alpaca_e3_copy.py /root/ft/train/iter_768.pth /root/ft/huggingface --fp32 --max-shard-size 2GB
```
#### 2.5.2 æ¨¡å‹æ•´åˆ
æˆ‘ä»¬é€šè¿‡è§†é¢‘è¯¾ç¨‹çš„å­¦ä¹ å¯ä»¥äº†è§£åˆ°ï¼Œå¯¹äº LoRA æˆ–è€… QLoRA å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å…¶å®å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢å¤–çš„å±‚ï¼ˆadapterï¼‰ã€‚é‚£ä¹ˆè®­ç»ƒå®Œçš„è¿™ä¸ªå±‚æœ€ç»ˆè¿˜æ˜¯è¦ä¸åŸæ¨¡å‹è¿›è¡Œç»„åˆæ‰èƒ½è¢«æ­£å¸¸çš„ä½¿ç”¨ã€‚

è€Œå¯¹äºå…¨é‡å¾®è°ƒçš„æ¨¡å‹ï¼ˆfullï¼‰å…¶å®æ˜¯ä¸éœ€è¦è¿›è¡Œæ•´åˆè¿™ä¸€æ­¥çš„ï¼Œå› ä¸ºå…¨é‡å¾®è°ƒä¿®æ”¹çš„æ˜¯åŸæ¨¡å‹çš„æƒé‡è€Œéå¾®è°ƒä¸€ä¸ªæ–°çš„ adapter ï¼Œå› æ­¤æ˜¯ä¸éœ€è¦è¿›è¡Œæ¨¡å‹æ•´åˆçš„ã€‚

<img src="https://github.com/InternLM/Tutorial/assets/108343727/dbb82ca8-e0ef-41db-a8a9-7d6958be6a96" width="300" height="300">


åœ¨ XTuner ä¸­ä¹Ÿæ˜¯æä¾›äº†ä¸€é”®æ•´åˆçš„æŒ‡ä»¤ï¼Œä½†æ˜¯åœ¨ä½¿ç”¨å‰æˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ä¸‰ä¸ªåœ°å€ï¼ŒåŒ…æ‹¬åŸæ¨¡å‹çš„åœ°å€ã€è®­ç»ƒå¥½çš„ adapter å±‚çš„åœ°å€ï¼ˆè½¬ä¸º Huggingface æ ¼å¼åä¿å­˜çš„éƒ¨åˆ†ï¼‰ä»¥åŠæœ€ç»ˆä¿å­˜çš„åœ°å€ã€‚
```bash
# åˆ›å»ºä¸€ä¸ªåä¸º final_model çš„æ–‡ä»¶å¤¹å­˜å‚¨æ•´åˆåçš„æ¨¡å‹æ–‡ä»¶
mkdir -p /root/ft/final_model

# è§£å†³ä¸€ä¸‹çº¿ç¨‹å†²çªçš„ Bug 
export MKL_SERVICE_FORCE_INTEL=1

# è¿›è¡Œæ¨¡å‹æ•´åˆ
# xtuner convert merge  ${NAME_OR_PATH_TO_LLM} ${NAME_OR_PATH_TO_ADAPTER} ${SAVE_PATH} 
xtuner convert merge /root/ft/model /root/ft/huggingface /root/ft/final_model
```
é‚£é™¤äº†ä»¥ä¸Šçš„ä¸‰ä¸ªåŸºæœ¬å‚æ•°ä»¥å¤–ï¼Œå…¶å®åœ¨æ¨¡å‹æ•´åˆè¿™ä¸€æ­¥è¿˜æ˜¯å…¶ä»–å¾ˆå¤šçš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
| å‚æ•°å | è§£é‡Š |
| ------------------- | ------------------------------------------------------ |
| --max-shard-size {GB} | ä»£è¡¨æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§çš„å¤§å°ï¼ˆé»˜è®¤ä¸º2GBï¼‰                |
| --device {device_name} | è¿™é‡ŒæŒ‡çš„å°±æ˜¯deviceçš„åç§°ï¼Œå¯é€‰æ‹©çš„æœ‰cudaã€cpuå’Œautoï¼Œé»˜è®¤ä¸ºcudaå³ä½¿ç”¨gpuè¿›è¡Œè¿ç®— |
| --is-clip | è¿™ä¸ªå‚æ•°ä¸»è¦ç”¨äºç¡®å®šæ¨¡å‹æ˜¯ä¸æ˜¯CLIPæ¨¡å‹ï¼Œå‡å¦‚æ˜¯çš„è¯å°±è¦åŠ ä¸Šï¼Œä¸æ˜¯å°±ä¸éœ€è¦æ·»åŠ  |

> CLIPï¼ˆContrastive Languageâ€“Image Pre-trainingï¼‰æ¨¡å‹æ˜¯ OpenAI å¼€å‘çš„ä¸€ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿç†è§£å›¾åƒå’Œæè¿°å®ƒä»¬çš„æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ã€‚CLIP é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå­¦ä¹ å›¾åƒå’Œå¯¹åº”æ–‡æœ¬ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œå®ç°äº†å¯¹å›¾åƒå†…å®¹çš„ç†è§£å’Œåˆ†ç±»ï¼Œç”šè‡³èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚
åœ¨æ¨¡å‹æ•´åˆå®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹åˆ° final_model æ–‡ä»¶å¤¹é‡Œç”Ÿæˆäº†å’ŒåŸæ¨¡å‹æ–‡ä»¶å¤¹éå¸¸è¿‘ä¼¼çš„å†…å®¹ï¼ŒåŒ…æ‹¬äº†åˆ†è¯å™¨ã€æƒé‡æ–‡ä»¶ã€é…ç½®ä¿¡æ¯ç­‰ç­‰ã€‚å½“æˆ‘ä»¬æ•´åˆå®Œæˆåï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿæ­£å¸¸çš„è°ƒç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•äº†ã€‚

æ•´åˆå®Œæˆåå¯ä»¥æŸ¥çœ‹åœ¨ final_model æ–‡ä»¶å¤¹ä¸‹çš„å†…å®¹ã€‚
```
|-- final_model/
    |-- tokenizer.model
    |-- config.json
    |-- pytorch_model.bin.index.json
    |-- pytorch_model-00001-of-00002.bin
    |-- tokenization_internlm2.py
    |-- tokenizer_config.json
    |-- special_tokens_map.json
    |-- pytorch_model-00002-of-00002.bin
    |-- modeling_internlm2.py
    |-- configuration_internlm2.py
    |-- tokenizer.json
    |-- generation_config.json
    |-- tokenization_internlm2_fast.py
```

#### 2.5.3 å¯¹è¯æµ‹è¯•
åœ¨ XTuner ä¸­ä¹Ÿç›´æ¥çš„æä¾›äº†ä¸€å¥—åŸºäº transformers çš„å¯¹è¯ä»£ç ï¼Œè®©æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯ä¸ Huggingface æ ¼å¼çš„æ¨¡å‹è¿›è¡Œå¯¹è¯æ“ä½œã€‚æˆ‘ä»¬åªéœ€è¦å‡†å¤‡æˆ‘ä»¬åˆšåˆšè½¬æ¢å¥½çš„æ¨¡å‹è·¯å¾„å¹¶é€‰æ‹©å¯¹åº”çš„æç¤ºè¯æ¨¡ç‰ˆï¼ˆprompt-templateï¼‰å³å¯è¿›è¡Œå¯¹è¯ã€‚å‡å¦‚ prompt-template é€‰æ‹©æœ‰è¯¯ï¼Œå¾ˆæœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ­£ç¡®çš„è¿›è¡Œå›å¤ã€‚

> æƒ³è¦äº†è§£å…·ä½“æ¨¡å‹çš„ prompt-template æˆ–è€… XTuner é‡Œæ”¯æŒçš„ prompt-tempolateï¼Œå¯ä»¥åˆ° XTuner æºç ä¸­çš„ `xtuner/utils/templates.py` è¿™ä¸ªæ–‡ä»¶ä¸­è¿›è¡ŒæŸ¥æ‰¾ã€‚

```Bash
# ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/final_model --prompt-template internlm2_chat
```
æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›ç®€å•çš„æµ‹è¯•æ¥çœ‹çœ‹å¾®è°ƒåçš„æ¨¡å‹çš„èƒ½åŠ›ã€‚
> å‡å¦‚æˆ‘ä»¬æƒ³è¦è¾“å…¥å†…å®¹éœ€è¦åœ¨è¾“å…¥æ–‡å­—åæ•²å‡»ä¸¤ä¸‹å›è½¦ï¼Œå‡å¦‚æˆ‘ä»¬æƒ³æ¸…æ¥šå†å²è®°å½•éœ€è¦è¾“å…¥ RESETï¼Œå‡å¦‚æˆ‘ä»¬æƒ³è¦é€€å‡ºåˆ™éœ€è¦è¾“å…¥ EXITã€‚
```

é‚£å¯¹äº `xtuner chat` è¿™ä¸ªæŒ‡ä»¤è€Œè¨€ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„å‚æ•°å¯ä»¥è¿›è¡Œè®¾ç½®çš„ï¼ŒåŒ…æ‹¬ï¼š

| å¯åŠ¨å‚æ•°              | è§£é‡Š                                                               |
|-----------------------|--------------------------------------------------------------------|
| --system              | æŒ‡å®šSYSTEMæ–‡æœ¬ï¼Œç”¨äºåœ¨å¯¹è¯ä¸­æ’å…¥ç‰¹å®šçš„ç³»ç»Ÿçº§ä¿¡æ¯                   |
| --system-template     | æŒ‡å®šSYSTEMæ¨¡æ¿ï¼Œç”¨äºè‡ªå®šä¹‰ç³»ç»Ÿä¿¡æ¯çš„æ¨¡æ¿                           |
| **--bits**            | æŒ‡å®šLLMè¿è¡Œæ—¶ä½¿ç”¨çš„ä½æ•°ï¼Œå†³å®šäº†å¤„ç†æ•°æ®æ—¶çš„ç²¾åº¦                     |
| --bot-name            | è®¾ç½®botçš„åç§°ï¼Œç”¨äºåœ¨å¯¹è¯æˆ–å…¶ä»–äº¤äº’ä¸­è¯†åˆ«bot                       |
| --with-plugins        | æŒ‡å®šåœ¨è¿è¡Œæ—¶è¦ä½¿ç”¨çš„æ’ä»¶åˆ—è¡¨ï¼Œç”¨äºæ‰©å±•æˆ–å¢å¼ºåŠŸèƒ½                   |
| **--no-streamer**     | å…³é—­æµå¼ä¼ è¾“æ¨¡å¼ï¼Œå¯¹äºéœ€è¦ä¸€æ¬¡æ€§å¤„ç†å…¨éƒ¨æ•°æ®çš„åœºæ™¯                 |
| **--lagent**          | å¯ç”¨lagentï¼Œç”¨äºç‰¹å®šçš„è¿è¡Œæ—¶ç¯å¢ƒæˆ–ä¼˜åŒ–                            |
| --command-stop-word   | è®¾ç½®å‘½ä»¤çš„åœæ­¢è¯ï¼Œå½“é‡åˆ°è¿™äº›è¯æ—¶åœæ­¢è§£æå‘½ä»¤                       |
| --answer-stop-word    | è®¾ç½®å›ç­”çš„åœæ­¢è¯ï¼Œå½“ç”Ÿæˆå›ç­”æ—¶é‡åˆ°è¿™äº›è¯åˆ™åœæ­¢                     |
| --offload-folder      | æŒ‡å®šå­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼Œç”¨äºåŠ è½½æˆ–å¸è½½æ¨¡å‹æƒé‡                   |
| --max-new-tokens      | è®¾ç½®ç”Ÿæˆæ–‡æœ¬æ—¶å…è®¸çš„æœ€å¤§tokenæ•°é‡ï¼Œæ§åˆ¶è¾“å‡ºé•¿åº¦                    |
| **--temperature**     | è®¾ç½®ç”Ÿæˆæ–‡æœ¬çš„æ¸©åº¦å€¼ï¼Œè¾ƒé«˜çš„å€¼ä¼šä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´å¤šæ ·ï¼Œè¾ƒä½çš„å€¼ä¼šä½¿æ–‡æœ¬æ›´ç¡®å®š |
| --top-k               | è®¾ç½®ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§      |
| --top-p               | è®¾ç½®ç´¯è®¡æ¦‚ç‡é˜ˆå€¼ï¼Œä»…ä¿ç•™æ¦‚ç‡ç´¯åŠ é«˜äºtop-pçš„æœ€å°æ ‡è®°é›†ï¼Œå½±å“ç”Ÿæˆæ–‡æœ¬çš„è¿è´¯æ€§ |
| --seed                | è®¾ç½®éšæœºç§å­ï¼Œç”¨äºç”Ÿæˆå¯é‡ç°çš„æ–‡æœ¬å†…å®¹                            |

é™¤äº†è¿™äº›å‚æ•°ä»¥å¤–å…¶å®è¿˜æœ‰ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°å°±æ˜¯ `--adapter` ï¼Œè¿™ä¸ªå‚æ•°ä¸»è¦çš„ä½œç”¨å°±æ˜¯å¯ä»¥åœ¨è½¬åŒ–åçš„ adapter å±‚ä¸åŸæ¨¡å‹æ•´åˆä¹‹å‰æ¥å¯¹è¯¥å±‚è¿›è¡Œæµ‹è¯•ã€‚ä½¿ç”¨è¿™ä¸ªé¢å¤–çš„å‚æ•°å¯¹è¯çš„æ¨¡å‹å’Œæ•´åˆåçš„æ¨¡å‹å‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¤ªå¤šçš„åŒºåˆ«ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹è¯•ä¸åŒçš„æƒé‡æ–‡ä»¶ç”Ÿæˆçš„ adapter æ¥æ‰¾åˆ°æœ€ä¼˜çš„ adapter è¿›è¡Œæœ€ç»ˆçš„æ¨¡å‹æ•´åˆå·¥ä½œã€‚
```bash
# ä½¿ç”¨ --adapter å‚æ•°ä¸å®Œæ•´çš„æ¨¡å‹è¿›è¡Œå¯¹è¯
xtuner chat /root/ft/model --adapter /root/ft/huggingface --prompt-template internlm2_chat
```

#### 2.5.4 Web demo éƒ¨ç½²

é™¤äº†åœ¨ç»ˆç«¯ä¸­å¯¹æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å…¶å®è¿˜å¯ä»¥åœ¨ç½‘é¡µç«¯çš„ demo è¿›è¡Œå¯¹è¯ã€‚

é‚£é¦–å…ˆæˆ‘ä»¬éœ€è¦å…ˆä¸‹è½½ç½‘é¡µç«¯ web demo æ‰€éœ€è¦çš„ä¾èµ–ã€‚

```bash
pip install streamlit==1.24.0
```

ä¸‹è½½ [InternLM](https://github.com/InternLM/InternLM) é¡¹ç›®ä»£ç ï¼ˆæ¬¢è¿Starï¼‰ï¼


```shell
# åˆ›å»ºå­˜æ”¾ InternLM æ–‡ä»¶çš„ä»£ç 
mkdir -p /root/ft/web_demo && cd /root/ft/web_demo

# æ‹‰å– InternLM æºæ–‡ä»¶
git clone https://github.com/InternLM/InternLM.git

# è¿›å…¥è¯¥åº“ä¸­
cd /root/ft/web_demo/InternLM
```

å°† `/root/ft/web_demo/InternLM/chat/web_demo.py` ä¸­çš„å†…å®¹æ›¿æ¢ä¸ºä»¥ä¸‹çš„ä»£ç ï¼ˆä¸æºä»£ç ç›¸æ¯”ï¼Œæ­¤å¤„ä¿®æ”¹äº†æ¨¡å‹è·¯å¾„å’Œåˆ†è¯å™¨è·¯å¾„ï¼Œå¹¶ä¸”ä¹Ÿåˆ é™¤äº† avatar åŠ system_prompt éƒ¨åˆ†çš„å†…å®¹ï¼ŒåŒæ—¶ä¸ cli ä¸­çš„è¶…å‚æ•°è¿›è¡Œäº†å¯¹é½ï¼‰ã€‚


```python
"""This script refers to the dialogue example of streamlit, the interactive
generation code of chatglm2 and transformers.

We mainly modified part of the code logic to adapt to the
generation of our model.
Please refer to these links below for more information:
    1. streamlit chat example:
        https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps
    2. chatglm2:
        https://github.com/THUDM/ChatGLM2-6B
    3. transformers:
        https://github.com/huggingface/transformers
Please run with the command `streamlit run path/to/web_demo.py
    --server.address=0.0.0.0 --server.port 7860`.
Using `python path/to/web_demo.py` may cause unknown problems.
"""
# isort: skip_file
import copy
import warnings
from dataclasses import asdict, dataclass
from typing import Callable, List, Optional

import streamlit as st
import torch
from torch import nn
from transformers.generation.utils import (LogitsProcessorList,
                                           StoppingCriteriaList)
from transformers.utils import logging

from transformers import AutoTokenizer, AutoModelForCausalLM  # isort: skip

logger = logging.get_logger(__name__)


@dataclass
class GenerationConfig:
    # this config is used for chat to provide more diversity
    max_length: int = 2048
    top_p: float = 0.75
    temperature: float = 0.1
    do_sample: bool = True
    repetition_penalty: float = 1.000


@torch.inference_mode()
def generate_interactive(
    model,
    tokenizer,
    prompt,
    generation_config: Optional[GenerationConfig] = None,
    logits_processor: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor],
                                                List[int]]] = None,
    additional_eos_token_id: Optional[int] = None,
    **kwargs,
):
    inputs = tokenizer([prompt], padding=True, return_tensors='pt')
    input_length = len(inputs['input_ids'][0])
    for k, v in inputs.items():
        inputs[k] = v.cuda()
    input_ids = inputs['input_ids']
    _, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]
    if generation_config is None:
        generation_config = model.generation_config
    generation_config = copy.deepcopy(generation_config)
    model_kwargs = generation_config.update(**kwargs)
    bos_token_id, eos_token_id = (  # noqa: F841  # pylint: disable=W0612
        generation_config.bos_token_id,
        generation_config.eos_token_id,
    )
    if isinstance(eos_token_id, int):
        eos_token_id = [eos_token_id]
    if additional_eos_token_id is not None:
        eos_token_id.append(additional_eos_token_id)
    has_default_max_length = kwargs.get(
        'max_length') is None and generation_config.max_length is not None
    if has_default_max_length and generation_config.max_new_tokens is None:
        warnings.warn(
            f"Using 'max_length''s default ({repr(generation_config.max_length)}) \
                to control the generation length. "
            'This behaviour is deprecated and will be removed from the \
                config in v5 of Transformers -- we'
            ' recommend using `max_new_tokens` to control the maximum \
                length of the generation.',
            UserWarning,
        )
    elif generation_config.max_new_tokens is not None:
        generation_config.max_length = generation_config.max_new_tokens + \
            input_ids_seq_length
        if not has_default_max_length:
            logger.warn(  # pylint: disable=W4902
                f"Both 'max_new_tokens' (={generation_config.max_new_tokens}) "
                f"and 'max_length'(={generation_config.max_length}) seem to "
                "have been set. 'max_new_tokens' will take precedence. "
                'Please refer to the documentation for more information. '
                '(https://huggingface.co/docs/transformers/main/'
                'en/main_classes/text_generation)',
                UserWarning,
            )

    if input_ids_seq_length >= generation_config.max_length:
        input_ids_string = 'input_ids'
        logger.warning(
            f"Input length of {input_ids_string} is {input_ids_seq_length}, "
            f"but 'max_length' is set to {generation_config.max_length}. "
            'This can lead to unexpected behavior. You should consider'
            " increasing 'max_new_tokens'.")

    # 2. Set generation parameters if not already defined
    logits_processor = logits_processor if logits_processor is not None \
        else LogitsProcessorList()
    stopping_criteria = stopping_criteria if stopping_criteria is not None \
        else StoppingCriteriaList()

    logits_processor = model._get_logits_processor(
        generation_config=generation_config,
        input_ids_seq_length=input_ids_seq_length,
        encoder_input_ids=input_ids,
        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
        logits_processor=logits_processor,
    )

    stopping_criteria = model._get_stopping_criteria(
        generation_config=generation_config,
        stopping_criteria=stopping_criteria)
    logits_warper = model._get_logits_warper(generation_config)

    unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
    scores = None
    while True:
        model_inputs = model.prepare_inputs_for_generation(
            input_ids, **model_kwargs)
        # forward pass to get next token
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_attentions=False,
            output_hidden_states=False,
        )

        next_token_logits = outputs.logits[:, -1, :]

        # pre-process distribution
        next_token_scores = logits_processor(input_ids, next_token_logits)
        next_token_scores = logits_warper(input_ids, next_token_scores)

        # sample
        probs = nn.functional.softmax(next_token_scores, dim=-1)
        if generation_config.do_sample:
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            next_tokens = torch.argmax(probs, dim=-1)

        # update generated ids, model inputs, and length for next step
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        model_kwargs = model._update_model_kwargs_for_generation(
            outputs, model_kwargs, is_encoder_decoder=False)
        unfinished_sequences = unfinished_sequences.mul(
            (min(next_tokens != i for i in eos_token_id)).long())

        output_token_ids = input_ids[0].cpu().tolist()
        output_token_ids = output_token_ids[input_length:]
        for each_eos_token_id in eos_token_id:
            if output_token_ids[-1] == each_eos_token_id:
                output_token_ids = output_token_ids[:-1]
        response = tokenizer.decode(output_token_ids)

        yield response
        # stop when each sentence is finished
        # or if we exceed the maximum length
        if unfinished_sequences.max() == 0 or stopping_criteria(
                input_ids, scores):
            break


def on_btn_click():
    del st.session_state.messages


@st.cache_resource
def load_model():
    model = (AutoModelForCausalLM.from_pretrained('/root/ft/final_model',
                                                  trust_remote_code=True).to(
                                                      torch.bfloat16).cuda())
    tokenizer = AutoTokenizer.from_pretrained('/root/ft/final_model',
                                              trust_remote_code=True)
    return model, tokenizer


def prepare_generation_config():
    with st.sidebar:
        max_length = st.slider('Max Length',
                               min_value=8,
                               max_value=32768,
                               value=2048)
        top_p = st.slider('Top P', 0.0, 1.0, 0.75, step=0.01)
        temperature = st.slider('Temperature', 0.0, 1.0, 0.1, step=0.01)
        st.button('Clear Chat History', on_click=on_btn_click)

    generation_config = GenerationConfig(max_length=max_length,
                                         top_p=top_p,
                                         temperature=temperature)

    return generation_config


user_prompt = '<|im_start|>user\n{user}<|im_end|>\n'
robot_prompt = '<|im_start|>assistant\n{robot}<|im_end|>\n'
cur_query_prompt = '<|im_start|>user\n{user}<|im_end|>\n\
    <|im_start|>assistant\n'


def combine_history(prompt):
    messages = st.session_state.messages
    meta_instruction = ('')
    total_prompt = f"<s><|im_start|>system\n{meta_instruction}<|im_end|>\n"
    for message in messages:
        cur_content = message['content']
        if message['role'] == 'user':
            cur_prompt = user_prompt.format(user=cur_content)
        elif message['role'] == 'robot':
            cur_prompt = robot_prompt.format(robot=cur_content)
        else:
            raise RuntimeError
        total_prompt += cur_prompt
    total_prompt = total_prompt + cur_query_prompt.format(user=prompt)
    return total_prompt


def main():
    # torch.cuda.empty_cache()
    print('load model begin.')
    model, tokenizer = load_model()
    print('load model end.')


    st.title('InternLM2-Chat-1.8B')

    generation_config = prepare_generation_config()

    # Initialize chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message['role'], avatar=message.get('avatar')):
            st.markdown(message['content'])

    # Accept user input
    if prompt := st.chat_input('What is up?'):
        # Display user message in chat message container
        with st.chat_message('user'):
            st.markdown(prompt)
        real_prompt = combine_history(prompt)
        # Add user message to chat history
        st.session_state.messages.append({
            'role': 'user',
            'content': prompt,
        })

        with st.chat_message('robot'):
            message_placeholder = st.empty()
            for cur_response in generate_interactive(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=real_prompt,
                    additional_eos_token_id=92542,
                    **asdict(generation_config),
            ):
                # Display robot response in chat message container
                message_placeholder.markdown(cur_response + 'â–Œ')
            message_placeholder.markdown(cur_response)
        # Add robot response to chat history
        st.session_state.messages.append({
            'role': 'robot',
            'content': cur_response,  # pylint: disable=undefined-loop-variable
        })
        torch.cuda.empty_cache()


if __name__ == '__main__':
    main()
```

åœ¨è¿è¡Œå‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„å°±æ˜¯å°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚é‚£é¦–å…ˆæˆ‘ä»¬ä½¿ç”¨å¿«æ·é”®ç»„åˆ `Windows + R`ï¼ˆWindows å³å¼€å§‹èœå•é”®ï¼‰æ‰“å¼€æŒ‡ä»¤ç•Œé¢ï¼Œå¹¶è¾“å…¥å‘½ä»¤ï¼ŒæŒ‰ä¸‹å›è½¦é”®ã€‚ï¼ˆMac ç”¨æˆ·æ‰“å¼€ç»ˆç«¯å³å¯ï¼‰


æ‰“å¼€ PowerShell åï¼Œå…ˆæŸ¥è¯¢ç«¯å£ï¼Œå†æ ¹æ®ç«¯å£é”®å…¥å‘½ä»¤ ï¼ˆä¾‹å¦‚å›¾ä¸­ç«¯å£ç¤ºä¾‹ä¸º 38374ï¼‰ï¼š



ç„¶åæˆ‘ä»¬éœ€è¦åœ¨ PowerShell ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼ˆéœ€è¦æ›¿æ¢ä¸ºè‡ªå·±çš„ç«¯å£å·ï¼‰
```bash
# ä»æœ¬åœ°ä½¿ç”¨ ssh è¿æ¥ studio ç«¯å£
# å°†ä¸‹æ–¹ç«¯å£å· 38374 æ›¿æ¢æˆè‡ªå·±çš„ç«¯å£å·
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 38374
```


ä¹‹åæˆ‘ä»¬éœ€è¦è¾“å…¥ä»¥ä¸‹å‘½ä»¤è¿è¡Œ `/root/personal_assistant/code/InternLM` ç›®å½•ä¸‹çš„ `web_demo.py` æ–‡ä»¶ã€‚

```bash
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

> æ³¨æ„ï¼šè¦åœ¨æµè§ˆå™¨æ‰“å¼€ `http://127.0.0.1:6006` é¡µé¢åï¼Œæ¨¡å‹æ‰ä¼šåŠ è½½ã€‚

æ‰“å¼€ [http://127.0.0.1:6006](http://127.0.0.1:6006) åï¼Œç­‰å¾…åŠ è½½å®Œæˆå³å¯è¿›è¡Œå¯¹è¯ï¼Œé”®å…¥å†…å®¹ç¤ºä¾‹å¦‚ä¸‹ï¼š

    è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±


å‡å¦‚æˆ‘ä»¬è¿˜æƒ³å’ŒåŸæ¥çš„ InternLM2-Chat-1.8B æ¨¡å‹å¯¹è¯ï¼ˆå³åœ¨ `/root/ft/model` è¿™é‡Œçš„æ¨¡å‹å¯¹è¯ï¼‰ï¼Œæˆ‘ä»¬å…¶å®åªéœ€è¦ä¿®æ”¹183è¡Œå’Œ186è¡Œçš„æ–‡ä»¶åœ°å€å³å¯ã€‚

```diff
# ä¿®æ”¹æ¨¡å‹åœ°å€ï¼ˆç¬¬183è¡Œï¼‰
- model = (AutoModelForCausalLM.from_pretrained('/root/ft/final_model',
+ model = (AutoModelForCausalLM.from_pretrained('/root/ft/model',

# ä¿®æ”¹åˆ†è¯å™¨åœ°å€ï¼ˆç¬¬186è¡Œï¼‰
- tokenizer = AutoTokenizer.from_pretrained('/root/ft/final_model',
+ tokenizer = AutoTokenizer.from_pretrained('/root/ft/model',
```
ç„¶åä½¿ç”¨ä¸Šæ–¹åŒæ ·çš„å‘½ä»¤å³å¯è¿è¡Œã€‚

```bash
streamlit run /root/ft/web_demo/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```






å‚è€ƒå®˜æ–¹æ–‡æ¡£https://github.com/InternLM/xtuner/blob/main/docs/en/user_guides/dataset_format.md



åœ¨dataç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªgenerate_data.pyæ–‡ä»¶ï¼Œå°†ä»¥ä¸‹ä»£ç å¤åˆ¶è¿›å»ï¼Œç„¶åè¿è¡Œè¯¥è„šæœ¬å³å¯ç”Ÿæˆæ•°æ®é›†ã€‚







è¿™é‡Œè¿›è¡Œå¤§é‡çš„å¢é‡å¾®è°ƒï¼Œæ„å»ºçˆ±è‰çš„è¯­æ°”ä¸çŸ¥è¯†èƒŒæ™¯



å¯¹äºå…¶ä»–å¾®è°ƒæ–¹æ³•ï¼Œå»ºè®®é‡‡ç”¨ä»¥ä¸‹å‘½ä»¤è·å–åˆ—è¡¨ï¼ŒæŸ¥æ‰¾ç›¸å…³æ–‡ä»¶ï¼š

    xtuner list-cfg -p internlm
å…¶ä¸­-p ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹ internlm ä¸º Xtuner æ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚å¦‚æœæ‰€æä¾›çš„é…ç½®æ–‡ä»¶ä¸èƒ½æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œè¯·å¯¼å‡ºæ‰€æä¾›çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œç›¸åº”æ›´æ”¹ï¼š

    xtuner copy-cfg ${CONFIG_NAME} ${SAVE_DIR}
ä¾‹å¦‚é€šè¿‡ä¸‹åˆ—å‘½ä»¤å°†åä¸º internlm_7b_qlora_oasst1_e3 çš„ config å¯¼å‡ºè‡³å½“å‰ç›®å½•ä¸‹ï¼š



    xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .


ä¿®æ”¹æ‹·è´åçš„æ–‡ä»¶internlm_chat_7b_qlora_oasst1_e3_copy.pyï¼Œä¿®æ”¹ä¸‹è¿°ä½ç½®ï¼š


    pretrained_model_name_or_path = '/root/ft-oasst1/internlm-chat-7b'


    data_path = '/root/ft-oasst1/data/personal_assistant.json'


    max_length = 1024


    batch_size = 2


    max_epochs = 3


    evaluation_freq = 5


    evaluation_inputs = [ 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»' ]    #æŒ‰å®é™…æƒ…å†µä¿®æ”¹



    dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path))
    dataset_map_fn=None





ä½¿ç”¨tmux

    apt update -y

    apt install tmux -y

    tmux new -s Elysia

    tmux attach -t Elysia
    é€€å‡ºctrl B +D


    
  

    

 ç½‘é¡µDEMO

    pip install streamlit==1.24.0
 åˆ›å»ºcodeæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾InternLMé¡¹ç›®ä»£ç 
 
    mkdir /root/ft-oasst1/code && cd /root/ft-oasst1/code
    git clone https://github.com/InternLM/InternLM.git

åˆ‡æ¢ commit ç‰ˆæœ¬ï¼Œä¸æ•™ç¨‹ commit ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼Œå¯ä»¥è®©å¤§å®¶æ›´å¥½çš„å¤ç°ã€‚

    cd InternLM
    git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17


 å°† /root/ft-oasst1/code/InternLM/web_demo.py ä¸­ 29 è¡Œå’Œ 33 è¡Œçš„æ¨¡å‹è·¯å¾„æ›´æ¢ä¸ºMergeåå­˜æ”¾å‚æ•°çš„è·¯å¾„ /root/ft-oasst1/work_dirs/hf_merge

 è¿è¡Œ /root/personal_assistant/code/InternLM ç›®å½•ä¸‹çš„ web_demo.py æ–‡ä»¶ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨è¾“å…¥ http://127.0.0.1:6006 å³å¯ã€‚

    ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 37560


    streamlit run /root/ft-oasst1/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006

    













  0.2 ï¼ˆnowï¼‰å·²ä¼˜åŒ–æ•°æ®é›†ï¼Œå¢åŠ è¯­éŸ³ï¼Œå¢åŠ LangChainï¼Œæ”¯æŒå¤šè½®å¯¹è¯ã€‚

  
  0.3 è®¡åˆ’å¢åŠ LangChain æ­å»ºçˆ±è‰çš„çŸ¥è¯†åº“ï¼ˆå°åŠ©æ‰‹éœ€è¦æœºé‡ç®—åŠ›æ”¯æŒï¼‰



  
  0.4 è®¡åˆ’å¢åŠ  Agentæ¡†æ¶,ä½¿çˆ±è‰æ— æ‰€ä¸èƒ½å“¦

  

  

  0.5 åŠ å…¥  æ¢…æ¯”ä¹Œæ–¯
            ä¼Šç”¸
            æ¨±
            å¸•æœµ
            æ ¼è•¾ä¿®
            å
            æœˆä¸‹

            


  0.6 ä¿®å»ºå¤šäººç‰©å…±åŒå¯¹è¯â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”é»„é‡‘åº­é™¢èŒ¶è¯ä¼š

  
            


  0.7 è®¡åˆ’å¢åŠ æ–°äººç‰©    
  
1. Vivy -Fluorite Eyeâ€™s Song-    vivy    https://baike.baidu.com/item/%E8%96%87%E8%96%87%20-%E8%90%A4%E7%9F%B3%E7%9C%BC%E4%B9%8B%E6%AD%8C-/56618413


  
2. BEATLESS                      è•¾è¥¿äºš   https://baike.baidu.com/item/%E8%95%BE%E8%A5%BF%E4%BA%9A/22329569?fromModule=lemma_inlink




                        
                        Vivy æˆ‘æ˜¯ä½ çš„ç²‰ä¸å‘€
                        è•¾è¥¿äºšèµ›é«˜ï¼ŒèŒ¶é“èµ›é«˜ï¼Œå’•å™œå’•å™œ
  
    





